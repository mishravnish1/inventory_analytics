{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f7521d8-6d47-4a32-b963-a2d85644c84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "def get_logger(name=__name__):\n",
    "    logger = logging.getLogger(name)\n",
    "    if not logger.handlers:\n",
    "        # Remove any pre-existing handlers if necessary:\n",
    "        logger.propagate = False\n",
    "\n",
    "        # Create a stream handler logging to stdout\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s - %(name)s: %(message)s\")\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bfb566b-e4c6-4602-8f8a-e8400478cc8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def readKafkaStream(spark, serversString:str, topics:str):\n",
    "    try:\n",
    "        logger = get_logger(__name__)\n",
    "        logger.info(\"starting stream...\")\n",
    "        stream = spark\\\n",
    "            .readStream\\\n",
    "            .format(\"kafka\")\\\n",
    "            .option(\"kafka.bootstrap.servers\", serversString)\\\n",
    "            .option(\"subscribe\", topics)\\\n",
    "            .option(\"startingOffsets\", \"latest\")\\\n",
    "            .load()\n",
    "        logger.info(\"loaded stream\")\n",
    "        return stream \n",
    "    except Exception as e:\n",
    "        logger.error(\"In readKafkaStream() method -> \", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "320c0c7a-c73c-4d8f-882c-9409a13ab34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.streaming import DataStreamWriter, StreamingQuery\n",
    "from pyspark.sql import DataFrame\n",
    "# from ims.utils import get_logger\n",
    "\n",
    "now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "def writeStream(stream:DataFrame,\n",
    "                file_format:str = \"parquet\",\n",
    "                sink:str = f\"/tmp/stream_output_{now}\",\n",
    "                checkpoint_location:str = f\"/tmp/checkpoint_{now}\",\n",
    "                mode:str = \"append\",\n",
    "                query_name:str = f\"test_query{now}\") -> StreamingQuery:\n",
    "    try:        \n",
    "        logger = get_logger(__name__)\n",
    "        logger.info(f\"starting streaming({query_name}) write to {sink} in {file_format} format with {mode} mode...\")\n",
    "        write_stream = stream\\\n",
    "            .writeStream\\\n",
    "            .format(file_format)\\\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\\\n",
    "            .option(\"path\", sink)\\\n",
    "            .outputMode(mode)\\\n",
    "            .queryName(query_name)\\\n",
    "            .start()\n",
    "        # write_stream.awaitTermination()\n",
    "        logger.info(f\"writing stream ({query_name}) has started\")\n",
    "        return query_name\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to start streaming query {query_name} \", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7d52a5-5a1b-436a-9801-c29f4262935f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BOOTSTRAP_SERVERS = \"kafka-brokers-ip:9092\"\n",
    "schema_path = \"/FileStore/inventory_analytics/bronze\"\n",
    "\n",
    "def writeToBronze(spark):\n",
    "    cartReadStream = readKafkaStream(spark, BOOTSTRAP_SERVERS, topics=\"product_cart\")\n",
    "    cartWriteStream = writeStream(cartReadStream,\n",
    "                file_format=\"parquet\",\n",
    "                sink=f\"{schema_path}/product_cart\",\n",
    "                checkpoint_location=f\"{schema_path}/chkp/product_cart/\",\n",
    "                query_name=\"write_ims_cart_rate_bronze\")\n",
    "\n",
    "    clickReadStream = readKafkaStream(spark, BOOTSTRAP_SERVERS, topics=\"product_click\")\n",
    "    clickWriteStream = writeStream(clickReadStream,\n",
    "                file_format=\"parquet\",\n",
    "                sink=f\"{schema_path}/product_click\",\n",
    "                checkpoint_location=f\"{schema_path}/chkp/product_click/\",\n",
    "                query_name=\"write_ims_click_rate_bronze\")\n",
    "\n",
    "    \n",
    "    purchaseReadStream = readKafkaStream(spark, BOOTSTRAP_SERVERS, topics=\"product_purchase\")\n",
    "    purchaseWriteStream = writeStream(purchaseReadStream,\n",
    "                file_format=\"parquet\",\n",
    "                sink=f\"{schema_path}/product_purchase\",\n",
    "                checkpoint_location=f\"{schema_path}/chkp/product_purchase/\",\n",
    "                query_name=\"write_ims_purchase_rate_bronze\")\n",
    "    \n",
    "    return {\n",
    "        \"click\": clickWriteStream,\n",
    "        \"purchase\": purchaseWriteStream,\n",
    "        \"cart\": cartWriteStream\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f15bf50-521b-4c26-94ff-798c798424d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 08:30:24,018] INFO - __main__: starting stream...\n[2025-04-26 08:30:26,058] INFO - __main__: loaded stream\n[2025-04-26 08:30:26,060] INFO - __main__: starting streaming(write_ims_cart_rate_bronze) write to /FileStore/inventory_analytics/bronze/product_cart in parquet format with append mode...\n[2025-04-26 08:30:29,512] INFO - __main__: writing stream (write_ims_cart_rate_bronze) has started\n[2025-04-26 08:30:29,515] INFO - __main__: starting stream...\n[2025-04-26 08:30:29,597] INFO - __main__: loaded stream\n[2025-04-26 08:30:29,607] INFO - __main__: starting streaming(write_ims_click_rate_bronze) write to /FileStore/inventory_analytics/bronze/product_click in parquet format with append mode...\n[2025-04-26 08:30:30,754] INFO - __main__: writing stream (write_ims_click_rate_bronze) has started\n[2025-04-26 08:30:30,757] INFO - __main__: starting stream...\n[2025-04-26 08:30:30,879] INFO - __main__: loaded stream\n[2025-04-26 08:30:30,884] INFO - __main__: starting streaming(write_ims_purchase_rate_bronze) write to /FileStore/inventory_analytics/bronze/product_purchase in parquet format with append mode...\n[2025-04-26 08:30:32,232] INFO - __main__: writing stream (write_ims_purchase_rate_bronze) has started\n"
     ]
    }
   ],
   "source": [
    "streams = writeToBronze(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407930e3-8859-47f5-9248-eef8ef3fc6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started streaming click with id write_ims_click_rate_bronze\nstarted streaming purchase with id write_ims_purchase_rate_bronze\nstarted streaming cart with id write_ims_cart_rate_bronze\n"
     ]
    }
   ],
   "source": [
    "for query, stream in streams.items():\n",
    "    print(f\"started streaming {query} with id {stream}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66fcb97f-3f86-49f6-9339-35beb50a0097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
